{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно сохранены в pickle-файлы.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Параметры\n",
    "fs = 128  # Частота дискретизации\n",
    "n_subjects = 5  # Количество испытуемых\n",
    "channels = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', \n",
    "            'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
    "useful_channels = ['F7', 'F3', 'P7', 'O1', 'O2', 'P8', 'AF4']\n",
    "\n",
    "# Индексы полезных каналов\n",
    "use_channel_inds = [channels.index(c) for c in useful_channels if c in channels]\n",
    "\n",
    "# Пороговые значения для выделения состояний\n",
    "mkpt1 = int(fs * 10 * 60)  # 10 минут в отсчетах\n",
    "mkpt2 = int(fs * 20 * 60)  # 20 минут в отсчетах\n",
    "mkpt3 = 214540  # Конец записи (примерное значение)\n",
    "interval = mkpt3 - mkpt2\n",
    "\n",
    "# Карта соответствия испытуемого и используемых дней (тестовые дни начинаются с 3-го)\n",
    "subject_map = {\n",
    "    s: [i for i in range(int(7 * (s - 1)) + 3, \n",
    "                         int(7 * (s - 1)) + (5 if s != 5 else 4) + 3)\n",
    "        ]\n",
    "    for s in range(1, n_subjects + 1)\n",
    "}\n",
    "\n",
    "# Директория с данными\n",
    "inp_dir = 'data/EEG Data/'\n",
    "\n",
    "# Сохранение данных в pickle для каждого испытуемого\n",
    "for s in range(1, n_subjects + 1):\n",
    "    data = {'channels': useful_channels, 'fs': fs}\n",
    "    \n",
    "    for i, t in enumerate(subject_map[s]):\n",
    "        trial = {}\n",
    "        trial_data = loadmat(f'{inp_dir}eeg_record{t}.mat')\n",
    "        eeg = trial_data['o']['data'][0][0][:, 3:17]  # Извлекаем каналы\n",
    "        eeg = eeg[:, use_channel_inds]  # Отбор полезных каналов\n",
    "        \n",
    "        trial['focussed'] = eeg[:interval]\n",
    "        trial['unfocussed'] = eeg[mkpt1:mkpt1 + interval]\n",
    "        trial['drowsed'] = eeg[mkpt2:mkpt2 + interval]\n",
    "        data[f'trial_{i + 1}'] = trial\n",
    "    \n",
    "    with open(f'data/temp_files/subject_{s}.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Данные успешно сохранены в pickle-файлы.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2632608, 1, 7), Validation shape: (877536, 1, 7), Test shape: (877536, 1, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "\n",
    "# Загрузка данных из всех pickle-файлов\n",
    "X, y = [], []\n",
    "state_num = {'focussed': 0, 'unfocussed': 1, 'drowsed': 2}\n",
    "\n",
    "for file in glob.glob('data/temp_files/subject_*.pkl'):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        for trial_key, trial_data in data.items():\n",
    "            if 'trial' in trial_key:\n",
    "                for state, label in state_num.items():\n",
    "                    X.append(trial_data[state])\n",
    "                    y.append(np.full(trial_data[state].shape[0], label))\n",
    "\n",
    "# Преобразование в numpy массивы\n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)\n",
    "\n",
    "# Стандартизация данных\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Ресайз для подачи в LSTM (samples, timesteps, features)\n",
    "X = X.reshape(-1, 1, X.shape[1])\n",
    "\n",
    "# Разделение на тренировочную, валидационную и тестовую выборки\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}, Test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Study_projects_part_2\\brain_focus_signals\\impulse_egg_signals_project\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2ms/step - accuracy: 0.4916 - loss: 0.9928 - val_accuracy: 0.5818 - val_loss: 0.8696\n",
      "Epoch 2/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 2ms/step - accuracy: 0.5435 - loss: 0.9217 - val_accuracy: 0.5987 - val_loss: 0.8454\n",
      "Epoch 3/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 3ms/step - accuracy: 0.5558 - loss: 0.9065 - val_accuracy: 0.6087 - val_loss: 0.8294\n",
      "Epoch 4/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 3ms/step - accuracy: 0.5625 - loss: 0.8978 - val_accuracy: 0.6133 - val_loss: 0.8236\n",
      "Epoch 5/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 3ms/step - accuracy: 0.5664 - loss: 0.8913 - val_accuracy: 0.6156 - val_loss: 0.8187\n",
      "Epoch 6/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 3ms/step - accuracy: 0.5696 - loss: 0.8869 - val_accuracy: 0.6179 - val_loss: 0.8138\n",
      "Epoch 7/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 3ms/step - accuracy: 0.5717 - loss: 0.8832 - val_accuracy: 0.6204 - val_loss: 0.8094\n",
      "Epoch 8/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 3ms/step - accuracy: 0.5748 - loss: 0.8795 - val_accuracy: 0.6219 - val_loss: 0.8068\n",
      "Epoch 9/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 3ms/step - accuracy: 0.5755 - loss: 0.8784 - val_accuracy: 0.6238 - val_loss: 0.8046\n",
      "Epoch 10/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 3ms/step - accuracy: 0.5769 - loss: 0.8757 - val_accuracy: 0.6247 - val_loss: 0.8016\n",
      "Epoch 11/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 3ms/step - accuracy: 0.5773 - loss: 0.8758 - val_accuracy: 0.6245 - val_loss: 0.8020\n",
      "Epoch 12/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 3ms/step - accuracy: 0.5790 - loss: 0.8729 - val_accuracy: 0.6262 - val_loss: 0.7978\n",
      "Epoch 13/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 3ms/step - accuracy: 0.5797 - loss: 0.8712 - val_accuracy: 0.6257 - val_loss: 0.7983\n",
      "Epoch 14/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 3ms/step - accuracy: 0.5795 - loss: 0.8717 - val_accuracy: 0.6265 - val_loss: 0.7961\n",
      "Epoch 15/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 3ms/step - accuracy: 0.5813 - loss: 0.8694 - val_accuracy: 0.6264 - val_loss: 0.7968\n",
      "Epoch 16/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 3ms/step - accuracy: 0.5812 - loss: 0.8695 - val_accuracy: 0.6285 - val_loss: 0.7964\n",
      "Epoch 17/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 3ms/step - accuracy: 0.5817 - loss: 0.8684 - val_accuracy: 0.6286 - val_loss: 0.7948\n",
      "Epoch 18/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 3ms/step - accuracy: 0.5827 - loss: 0.8673 - val_accuracy: 0.6298 - val_loss: 0.7939\n",
      "Epoch 19/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2ms/step - accuracy: 0.5825 - loss: 0.8670 - val_accuracy: 0.6287 - val_loss: 0.7941\n",
      "Epoch 20/20\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 3ms/step - accuracy: 0.5829 - loss: 0.8672 - val_accuracy: 0.6310 - val_loss: 0.7920\n",
      "27423/27423 - 30s - 1ms/step - accuracy: 0.6300 - loss: 0.7926\n",
      "Test Loss: 0.7926, Test Accuracy: 0.6300\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Определение модели\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(LSTM(32, return_sequences=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))  # 3 класса: focussed, unfocussed, drowsed\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callback для ранней остановки\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Обучение модели\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Оценка модели на тестовой выборке\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27423/27423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Focussed       0.72      0.73      0.73    292512\n",
      "  Unfocussed       0.58      0.51      0.54    292512\n",
      "     Drowsed       0.58      0.65      0.62    292512\n",
      "\n",
      "    accuracy                           0.63    877536\n",
      "   macro avg       0.63      0.63      0.63    877536\n",
      "weighted avg       0.63      0.63      0.63    877536\n",
      "\n",
      "Confusion Matrix:\n",
      "[[213055  38825  40632]\n",
      " [ 47851 148674  95987]\n",
      " [ 33495  67914 191103]]\n"
     ]
    }
   ],
   "source": [
    "# Прогноз и оценка\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Focussed', 'Unfocussed', 'Drowsed']))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_8248\\4008889525.py:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  model.save('microservice_architecture\\model\\src\\lstm_model.h5')\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Сохранение обученной модели\n",
    "model.save('microservice_architecture\\model\\src\\lstm_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impulse_egg_signals_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
